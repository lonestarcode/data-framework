# Scrape-Text

A robust text scraping framework designed to collect and clean textual data from multiple sources. Part of the larger data science framework's data collection pipeline.

## ğŸ¯ Purpose

Scrape-Text serves as the data collection layer, focusing on:
- Multi-source text extraction (news, social media, documents)
- Initial text cleaning and validation
- Structured data storage
- Rate-limited and proxy-supported scraping

## ğŸ— Architecture

```
scrape-text/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ scrapers/                 # Source-specific scrapers
â”‚   â”‚   â”œâ”€â”€ news_scraper.py      # News website scraping
â”‚   â”‚   â”œâ”€â”€ social_scraper.py    # Social media content
â”‚   â”‚   â”œâ”€â”€ document_scraper.py  # Document processing
â”‚   â”‚   â””â”€â”€ api_collectors/      # API-based collectors
â”‚   â”‚       â”œâ”€â”€ twitter_api.py
â”‚   â”‚       â””â”€â”€ reddit_api.py
â”‚   â”œâ”€â”€ processors/              # Basic processing
â”‚   â”‚   â”œâ”€â”€ html_cleaner.py     # HTML removal
â”‚   â”‚   â”œâ”€â”€ text_validator.py   # Content validation
â”‚   â”‚   â””â”€â”€ metadata_extractor.py # Extract article metadata
â”‚   â””â”€â”€ utils/                  # Shared utilities
â”‚       â”œâ”€â”€ rate_limiter.py     # API rate limiting
â”‚       â”œâ”€â”€ proxy_manager.py    # Proxy rotation
â”‚       â””â”€â”€ url_validator.py    # URL validation
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                   # Original scraped content
â”‚   â””â”€â”€ processed/             # Cleaned text
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ scraper_config.yaml   # Scraping rules
â”‚   â””â”€â”€ api_config.yaml       # API credentials
â””â”€â”€ tests/
    â”œâ”€â”€ unit/
    â””â”€â”€ integration/
```

## ğŸš€ Features

### 1. News Scraping
```python
response = news_scraper.scrape('https://example.com/news')
# Returns: {'url': '...', 'title': '...', 'content': '...', 'metadata': {...}}
```

### 2. Social Media Collection
```python
tweets = twitter_collector.collect('#AI', max_results=100)
# Returns: [{'id': '...', 'text': '...', 'user': '...', 'timestamp': '...'}, ...]
```

### 3. Document Processing
```python
content = document_scraper.process('document.pdf')
# Returns: {'content': '...', 'pages': 5, 'metadata': {...}}
```

## ğŸ“‹ Requirements

```bash
pip install -r requirements.txt
```

Key dependencies:
- beautifulsoup4==4.12.2
- requests==2.31.0
- aiohttp==3.8.5
- tweepy==4.14.0
- pdfminer.six==20221105
- python-dotenv==1.0.0

## âš™ï¸ Configuration

### Scraper Configuration
```yaml
# config/scraper_config.yaml
news_sources:
  - name: "Tech News"
    url: "https://technews.com"
    selectors:
      article: "article.news-item"
      title: "h1.title"
      content: "div.content"
      date: "span.date"
    rate_limit: 1  # requests per second

social_sources:
  - platform: "twitter"
    search_terms:
      - "#technology"
      - "#AI"
    max_results: 100
    interval: 3600  # seconds between searches
```

## ğŸƒâ€â™‚ï¸ Usage

### Run Scraping Pipeline
```bash
# Run all scrapers
python scripts/run_scrapers.py

# Run specific scraper
python scripts/run_scrapers.py --source news
```

### Monitor Progress
```bash
# View scraping status
python scripts/check_status.py

# View error logs
python scripts/view_logs.py
```

## ğŸ”„ Integration Points

### Output Format
```json
{
    "id": "unique_id",
    "url": "source_url",
    "title": "article_title",
    "content": "cleaned_content",
    "metadata": {
        "author": "author_name",
        "date": "2023-10-20T10:00:00Z",
        "category": "technology"
    },
    "scrape_timestamp": "2023-10-20T10:05:00Z"
}
```

### Next Steps
- Data flows to analysis-text for:
  - Summarization
  - Sentiment analysis
  - Topic modeling
  - Bias detection

## ğŸ“Š Monitoring

### Health Checks
```bash
# Check scraper health
python monitoring/health_check.py

# View scraping metrics
python monitoring/view_metrics.py
```

### Key Metrics
- Scraping success rate
- Content validation rate
- API rate limit status
- Proxy rotation status

## ğŸ§ª Testing

```bash
# Run all tests
pytest

# Test specific scraper
pytest tests/unit/test_news_scraper.py
```

## ğŸ”’ Security

- API keys stored in environment variables
- Proxy support for anonymity
- Rate limiting for API compliance
- URL and content validation
- Error handling and logging

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Commit your changes
4. Push to the branch
5. Create a Pull Request

## ğŸ“ License

This project is licensed under the MIT License - see the LICENSE file for details.
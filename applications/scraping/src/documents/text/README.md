# Text-Scrape

Advanced text scraping module with configurable filters, processing pipelines, and robust error handling.

## ğŸ— Directory Structure

```
text-scrape/
â”œâ”€â”€ config/                      # Configuration files
â”‚   â”œâ”€â”€ filters/                # Filter configurations
â”‚   â”œâ”€â”€ monitoring/             # Dashboard settings
â”‚   â”œâ”€â”€ scraper/               # Scraper settings
â”‚   â””â”€â”€ storage/               # Storage rules
â”œâ”€â”€ data/                       # Data storage
â”‚   â”œâ”€â”€ cache/                 # Temporary data
â”‚   â”œâ”€â”€ processed/             # Processed text
â”‚   â””â”€â”€ raw/                   # Raw scraped data
â””â”€â”€ src/                        # Source code
    â”œâ”€â”€ scrapers/              # Data collection
    â”‚   â”œâ”€â”€ base_text_scraper.py  # Base text scraping functionality
    â”‚   â”œâ”€â”€ document_scraper.py   # Document format scraping
    â”‚   â”œâ”€â”€ news_scraper.py       # News content scraping
    â”‚   â””â”€â”€ social_scraper.py     # Social media scraping
    â”œâ”€â”€ cache/                 # Cache management
    â”œâ”€â”€ dashboard/             # Monitoring interface
    â”œâ”€â”€ error_handlers/        # Error management
    â”œâ”€â”€ filters/               # Content filtering
    â”‚   â”œâ”€â”€ advisory/         # Non-blocking filters
    â”‚   â””â”€â”€ prohibitive/      # Blocking filters
    â”œâ”€â”€ logging/              # Logging system
    â”œâ”€â”€ migrations/           # Database migrations
    â”œâ”€â”€ processors/           # Text processing
    â”œâ”€â”€ proxy_management/     # Proxy handling
    â”œâ”€â”€ rate_limiters/        # Request throttling
    â”œâ”€â”€ schemas/              # Data validation
    â”œâ”€â”€ storage_rules/        # Storage management
    â””â”€â”€ validators/           # Input validation
```

## ğŸš€ Features

### Data Collection
- **Document Scraping**: 
  - Supports PDF, DOCX, and TXT formats
  - Metadata extraction
  - Content parsing with error handling
- **News Scraping**: 
  - Article extraction
  - Title, content, and author parsing
  - Publication date detection
- **Social Media Scraping**:
  - Support for Twitter, LinkedIn, and Facebook
  - API-based content retrieval
  - Platform-specific data parsing

### Base Functionality
- **BaseTextScraper**:
  - Async HTTP session management
  - Common text cleaning utilities
  - HTML content extraction
  - Text validation methods

### Content Filtering
- **Advisory Filters**
  - Reliability assessment
  - Sentiment analysis
- **Prohibitive Filters**
  - Content validation
  - Quality checks

### Processing & Storage
- Text and metadata processing
- Configurable storage rules
- Cache management
- Database migrations

### Safety & Performance
- Rate limiting strategies
- Proxy management
- Error handling
- Input validation

### Monitoring
- Real-time dashboard
- Performance metrics
- Status monitoring
- Logging system

## ğŸ›  Setup

1. Install dependencies:
```bash
pip install -r requirements.txt
```

2. Configure settings:
- Update `config/scraper/config.yaml` for scraper settings
- Adjust `config/filters/filter_rules.yaml` for content filtering
- Modify `config/storage/storage_rules.yaml` for storage rules

3. Initialize database:
```bash
alembic upgrade head
```

## ğŸ”„ Usage

### Basic Scraping Example
```python
from src.scrapers.document_scraper import DocumentScraper
from src.scrapers.news_scraper import NewsScraper
from src.scrapers.social_scraper import SocialScraper

# Document scraping
async with DocumentScraper() as doc_scraper:
    docs = await doc_scraper.scrape({
        'url': 'https://example.com/document.pdf',
        'format': 'pdf'
    })

# News scraping
async with NewsScraper() as news_scraper:
    articles = await news_scraper.scrape({
        'url': 'https://news-site.com/article'
    })

# Social media scraping
async with SocialScraper() as social_scraper:
    posts = await social_scraper.scrape({
        'platform': 'twitter',
        'query': 'search term'
    })
```

## ğŸ“Š Data Flow

1. **Collection**: Scrapers gather raw text data
2. **Validation**: Input validators check data integrity
3. **Filtering**: Content passes through advisory and prohibitive filters
4. **Processing**: Text and metadata processors clean and structure data
5. **Storage**: Processed data is stored according to configured rules

## âš™ï¸ Configuration

- `filter_rules.yaml`: Define content filtering rules
- `config.yaml`: Configure scraper behavior
- `storage_rules.yaml`: Set data storage policies
- `dashboard.yaml`: Configure monitoring settings

## ğŸ”’ Safety Features

- Rate limiting prevents overloading sources
- Proxy management for distributed access
- Error handling for resilient operation
- Input validation for data integrity

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Commit changes
4. Push to the branch
5. Create a Pull Request




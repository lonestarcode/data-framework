# Data Science Framework

A comprehensive data science platform featuring advanced scraping, machine learning, and automated workflow capabilities. This framework provides a production-ready foundation for building and deploying data-driven applications with robust processing pipelines and monitoring systems.

## Core Capabilities

### Data Collection & Processing
- Multi-source data scraping (News, Facebook, Financial APIs)
- Real-time data streaming and processing
- Automated validation and cleaning pipelines
- Structured data storage and versioning

### Machine Learning & Analysis
- NLP-powered content analysis
- Market prediction models
- Automated trading algorithms
- Continuous model training and evaluation

### Deployment & Monitoring
- Docker containerization
- Kubernetes orchestration
- Cloud-native infrastructure (AWS CDK)
- Comprehensive monitoring and logging



## Data Workflow

### **1. Scraping**
- Scraping applications collect raw data from various sources (e.g., news websites, social media platforms, real estate platforms).
- Data is saved in a standardized format under the `raw/` directory for each application.

### **2. Analytics**
- Analytics applications process, clean, and analyze raw data collected by scrapers.
- The processed data is stored under the `processed/` directory, ready for advanced modeling or analysis.

### **3. Automated Workflows**
- Workflow applications, like the `content-generator`, use processed data to create actionable outputs (e.g., articles, reports, or trading decisions).

### **4. Media Processing**
- Non-textual data, such as images and audio, are analyzed for insights or integrated with textual data for comprehensive analytics.

---

## Example Pipeline

1. **Scraping:**
   - `news-scraper` collects raw articles and stores them in `raw/`.
   
2. **Analytics:**
   - `news-summary` processes the articles into summarized formats.
   - `text-analysis` provides sentiment and bias analysis for the summaries.

3. **Automated Workflow:**
   - `content-generator` combines summaries and analysis to produce polished articles.

4. **Output:**
   - The final articles are saved and made available for user interaction or publication.

---

## Key Features
- **Modularity:** Each application functions independently but integrates seamlessly into the overall framework.
- **Scalability:** New applications can be added to any category with minimal restructuring.
- **Traceability:** Each step in the pipeline is logged for transparency and debugging.

---

## Future Plans
- Add more media-processing capabilities, such as video analysis.
- Enhance integration between scraping and analytics for real-time insights.
- Expand automated workflows to support industry-specific applications.

---


*** While the framework consolidates and prepares data for machine learning analysis in a centralized data/ directory, it also retains project-specific data for user interaction. This dual approach ensures that the integrity and purpose of individual projects remain intact, enabling direct interaction and analysis without compromising the broader ML workflows***

Data Workflow: Multi-Purpose Data Management
	1.	Raw Data Retention
Each project maintains its own local database within its respective directory under projects/applications/. This ensures that:
	•	Users can directly interact with the raw, unprocessed data specific to the application.
	•	Data is accessible in its native structure and format for the intended functionality of the project.

	2.	Global Data Consolidation
A copy of the data flows into the global data/ directory for ML training and cross-project analytics:
	•	Raw data: Stored under data/raw/ for system-wide ingestion.
	•	Processed data: Standardized and cleaned, ensuring consistency across projects.
	•	Validated data: Tagged for production use in ML pipelines.
This allows applications to contribute to a larger data ecosystem while maintaining their operational independence.
	
  3.	Bidirectional Data Sharing
	•	From Projects to ML: Data pipelines automatically push data from project-specific databases into the global data/ directory for aggregation and analysis.
	•	From ML to Projects: Insights or predictions generated by ML models are stored in project directories for user interaction.

User Interaction with Raw Data

For each application, raw data remains accessible for its core functionality. Examples include:
	•	Facebook Marketplace Monitor: Users can view live marketplace listings directly from the local database.
	•	News Summarizer: Users can access detailed summaries or filter content interactively based on the raw news feeds collected.
	•	Options Trading Bot: Real-time options data and risk insights are served from the project database, ensuring accuracy and immediacy.


## Data Processing Pipeline

### 1. Data Collection
- Applications collect raw data through various scrapers and APIs
- Each application maintains its own collection logic while adhering to global standards

### 2. Data Consolidation
All data flows through a standardized directory structure:
```
data/
├── raw/        # Initial unprocessed data from all sources
├── processed/  # Cleaned and normalized datasets
└── validated/  # Production-ready data for ML models
```

### 3. Machine Learning Workflow
The `ml/` directory manages the complete ML lifecycle:
- Model training using validated datasets
- Evaluation metrics and performance tracking
- Model versioning and deployment
- Continuous training pipelines

### 4. Monitoring and Feedback
Integrated monitoring system tracks:
- Data quality metrics
- Processing pipeline performance
- Model accuracy and drift
- System resource utilization
- User feedback and corrections

### 5. Deployment
The `deployment/` directory contains:
- Docker configurations for containerization
- Kubernetes manifests for orchestration
- Cloud infrastructure templates (AWS CDK)
- CI/CD pipeline configurations
- Environment-specific settings


## Available Applications

### News Summarizer
- Automated news collection and summarization
- NLP-based content filtering
- LLM-powered summarization (GPT-4/Claude)
- User feedback system
- React/TypeScript frontend
- AWS infrastructure

### Facebook Marketplace Monitor
- Real-time listing monitoring
- Multi-category support
- Anti-detection measures
- Notification system
- Python 3.10+ compatible

### Property Analysis Tool
- Investment scenario modeling
- AI-powered market insights
- Interactive visualization
- Street view integration
- Export capabilities

### Options Trading Bot
- GPT-powered market analysis
- Real-time options data
- Risk management system
- TD Ameritrade integration
- Paper trading support

### Text Analysis Core
- Sentiment analysis
- Topic modeling
- Language detection
- Bias detection
- API integration

### Listing Management Bot
- Automated listing control
- Scheduled operations
- Web interface
- Real-time monitoring
- Flask backend

##  Directory Structure  

```
data-framework/
├── backend/                 # Django backend for orchestration
│   └── src/                # Source code for Django project
├── config/                 # Configuration files for environments
├── data/                   # Data storage
│   ├── raw/               # Raw data
│   ├── processed/         # Processed data
│   └── validated/         # Validated data
├── deployment/            # Deployment configurations
├── ml/                    # Machine learning workflows
│   ├── datasets/         # Datasets for training and testing
│   ├── models/           # Model directories
│   └── training_scripts/ # Training scripts
├── projects/              # Data scraping projects
│   └── applications/
│       ├── news-summary/         # News aggregation system
│       ├── facebook-market/      # Marketplace monitoring
│       ├── property-analysis/    # Real estate analysis tool
│       ├── options-trader/       # Options trading system
│       ├── text-analysis/        # NLP processing system
│       └── listing-bot/         # Listing automation system
└── scripts/              # Global utility scripts
```

## Prerequisites

### System Requirements
- Python 3.9+
- Node.js 16+
- Docker
- Kubernetes (optional)
- AWS Account (for cloud deployment)

### API Keys Required
- OpenAI API (GPT-4/Claude)
- TD Ameritrade API
- Facebook API (optional)
- News API services

## Installation

1. Clone the repository:
```bash
git clone https://github.com/yourusername/data-framework.git
cd data-framework
```

2. Set up a virtual environment:
```bash
python -m venv venv
source venv/bin/activate  # For Linux/Mac
venv\Scripts\activate     # For Windows
```

3. Install dependencies:
```bash
pip install -r requirements.txt
npm install  # For frontend components
```

4. Configure environment:
```bash
cp .env.example .env
# Edit .env with your API keys and configurations
```

5. Initialize the database:
```bash
python manage.py migrate
```

## Development

### Running Locally
```bash
# Backend
python manage.py runserver

# Frontend
npm start

# Docker
docker-compose up
```

### Testing
```bash
# Backend tests
pytest

# Frontend tests
npm test

# Integration tests
pytest tests/integration
```

## Deployment

### Docker Deployment
```bash
docker-compose -f docker-compose.prod.yml up -d
```

### Kubernetes Deployment
```bash
kubectl apply -f deployment/kubernetes/
```

### AWS Deployment
```bash
cd infrastructure
cdk deploy
```

## Documentation

Each application includes detailed documentation:
- [News Summarizer Documentation](projects/applications/news-summary/README.md)
- [Facebook Marketplace Documentation](projects/applications/facebook-market/README.md)
- [Property Analysis Documentation](projects/applications/property-analysis/README.md)
- [Options Trading Documentation](projects/applications/options-trader/README.md)
- [Text Analysis Documentation](projects/applications/text-analysis/README.md)
- [Listing Bot Documentation](projects/applications/listing-bot/README.md)

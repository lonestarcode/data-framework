I'll explain how the ML folder interacts with the global database architecture and data flow:

# ML Folder Data Integration

## 1. Data Sources
The ML folder receives data from the global database that originates from various applications:


````285:319:README.md
Applications will have their own local databases, but the data will also be stored in a global database for ML and analytics.

1. **Local Database (Application-Specific)**
- Each application (scraping, analytics, workflows) maintains its own database in its directory
- Purpose:
  - Maintains data in its original format
  - Allows direct application-specific operations
  - Preserves raw data integrity
- Example structure:
```python
# apps/scraping/models.py
class RawScrapedData(models.Model):
    source = models.CharField(max_length=100)  # e.g., "facebook", "twitter"
    raw_content = models.JSONField()  # Keeps original format
    scraped_at = models.DateTimeField(auto_now_add=True)
    metadata = models.JSONField()  # Source-specific metadata
```

2. **Global Database (ML & Analytics)**
- Centralized data storage in `data/` directory
- Purpose:
  - ML model training
  - Cross-project analytics
  - System-wide data access
- Example structure:
```python
# apps/analytics/models.py
class GlobalMLData(models.Model):
    source_app = models.CharField(max_length=100)  # Origin application
    data_type = models.CharField(max_length=50)    # "text", "market", etc.
    processed_content = models.JSONField()         # Standardized format
    created_at = models.DateTimeField(auto_now_add=True)
    features = models.JSONField()                  # ML-ready features
```

````


## 2. Database Structure

### MongoDB Integration
- **Raw Training Data**
  - Unstructured text from news summarizer
  - Social media content
  - Market data streams
  - Model outputs and predictions

### SQL Integration
- **Structured Training Data**
  - Validated market metrics
  - User interaction data
  - Performance statistics
  - Model evaluation results

## 3. Data Flow Implementation

```python:ml/data_manager.py
class MLDataManager:
    def __init__(self):
        self.mongo_client = MongoClient(settings.MONGODB_URI)
        self.sql_engine = create_engine(settings.SQL_URI)
        
    async def fetch_training_data(self, data_type: str, time_range: dict):
        """Fetch training data from both databases based on type"""
        if data_type in ['text', 'social', 'raw_market']:
            # Fetch unstructured data from MongoDB
            return await self._fetch_mongo_data(data_type, time_range)
        else:
            # Fetch structured data from SQL
            return await self._fetch_sql_data(data_type, time_range)
    
    async def store_model_results(self, results: dict):
        """Store model outputs in appropriate database"""
        if results['type'] in ['predictions', 'embeddings']:
            # Store flexible outputs in MongoDB
            await self._store_mongo_results(results)
        else:
            # Store structured metrics in SQL
            await self._store_sql_results(results)
```

## 4. Training Pipeline Integration

```python:ml/training/base_trainer.py
class BaseModelTrainer:
    def __init__(self):
        self.data_manager = MLDataManager()
        
    async def prepare_training_data(self):
        # Get validated data from global database
        raw_data = await self.data_manager.fetch_training_data(
            data_type=self.config.data_type,
            time_range=self.config.time_range
        )
        
        # Process for training
        return self.preprocessor.transform(raw_data)
    
    async def save_model_metrics(self, metrics: dict):
        # Store structured metrics in SQL
        await self.data_manager.store_model_results({
            'type': 'metrics',
            'data': metrics
        })
```

## 5. Configuration

```yaml:ml/config/database_config.yaml
mongodb:
  collections:
    training_data:
      - text_corpus
      - market_data
      - social_content
    model_outputs:
      - embeddings
      - predictions
      - intermediate_results

sql:
  tables:
    metrics:
      - model_performance
      - validation_results
    structured_data:
      - market_metrics
      - user_interactions
```

## 6. Key Integration Points

The ML folder follows the bidirectional data sharing pattern described in:


```505:512:README.md
Bidirectional Data Sharing:
	•	From MongoDB to SQL:
	•	After raw data is collected and stored in MongoDB, it can be processed and moved to SQL for validation and analytics.
	•	Example: Scraped real estate data stored in MongoDB is cleaned and normalized, then transferred to SQL for investment scenario modeling.
	•	From SQL to MongoDB:
	•	Predictions or insights generated by ML models stored in SQL can be pushed back into MongoDB for use in real-time user-facing applications.
	•	Example: Storing trading recommendations in MongoDB to be served to a web app.

```


And implements the database strategy outlined in:


```472:484:README.md

MongoDB:
	•	NLP Content Analysis:
	•	MongoDB is ideal for storing and querying large volumes of text data for NLP tasks like sentiment and bias analysis.
	•	Example: Storing summarized news content with NLP analysis outputs.
	•	Training Data Aggregation:
	•	MongoDB can consolidate unstructured training datasets (e.g., raw text, images, logs) across multiple sources for machine learning pipelines.

SQL:
	•	Processed and Validated Data:
	•	SQL is better suited for storing processed and validated datasets used in machine learning models.
	•	Example: Storing normalized data for market prediction models in relational tables.
	•	Performance Tracking:
```


## 7. Benefits
1. **Efficient Data Access**
   - Unstructured data from MongoDB for flexible training
   - Structured metrics from SQL for performance tracking
   - Optimized query patterns for each data type

2. **Data Consistency**
   - Validated datasets from global database
   - Standardized preprocessing
   - Consistent evaluation metrics

3. **Scalability**
   - Independent scaling of training processes
   - Distributed data access
   - Resource optimization

This architecture ensures that the ML folder can efficiently access and utilize the global database while maintaining data integrity and optimal performance for different types of machine learning tasks.
